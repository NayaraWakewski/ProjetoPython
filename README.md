# ![logo](logo_gov_ce.png)

# Projeto An√°lise Conv√™nios e Contratos do Governo do Estado do Cear√°

Este reposit√≥rio cont√©m todos os c√≥digos e documenta√ß√£o para o projeto de engenharia de dados e an√°lise de conv√™nios e contratos do Governo do Cear√°, extra√≠dos do Portal de Dados Abertos.

## √çndice

- [**Engenharia de Dados com Airflow**](#engenharia-de-dados-com-airflow)
    - [Airflow e Docker](#airflow-e-docker)
    - [Banco de Dados PostgreSQL com DBeaver](#banco-de-dados-postgresql-com-dbeaver)
    - [DAGs do Airflow](#dags-do-airflow)
    - [Execu√ß√£o Manual e Agendamento](#execu√ß√£o-manual-e-agendamento)
    - [Automa√ß√£o do Pipeline de Dados com Airflow DAGs](#automa√ß√£o-do-pipeline-de-dados-com-airflow-dags)
- [**Desenvolvimento dos Scripts Python para Automa√ß√£o de Tarefas**](#desenvolvimento-dos-scripts-python-para-automa√ß√£o-de-tarefas)
    - [Caracter√≠sticas dos Scripts](#caracter√≠sticas-dos-scripts)
    - [Notebooks Jupyter para An√°lise e Educa√ß√£o](#notebooks-jupyter-para-an√°lise-e-educa√ß√£o)
- [**Dicion√°rio de Dados Stage**](#dicion√°rio-de-dados-stage)
- [**Cria√ß√£o do Banco de Dados Airflow (Schema Stage)**](#cria√ß√£o-do-banco-de-dados-airflow-schema-stage)
- [**An√°lise Explorat√≥ria Inicial no Banco de Dados Stage**](#an√°lise-explorat√≥ria-inicial-no-banco-de-dados-stage)
- [**Processo de ETL no Banco de Dados Stage**](#processo-de-etl-no-banco-de-dados-stage)
- [**Dicion√°rio de Dados DW**](#dicion√°rio-de-dados-dw)
- [**Modelagem de Dados do Data Warehouse**](#modelagem-de-dados-do-data-warehouse)
- [**Estrutura√ß√£o do Data Warehouse (Schema DW)**](#estrutura√ß√£o-do-data-warehouse-schema-dw)
- [**An√°lise Explorat√≥ria e Estat√≠stica DW**](#an√°lise-explorat√≥ria-e-estat√≠stica-dw)
- [**Data Visualization**](#data-visualization)
- [**Licen√ßa**](#licen√ßa)
- [**Como usar este reposit√≥rio**](#como-usar-este-reposit√≥rio)
- [**Equipe**](#team)
- [**üéÅ Express√µes de Gratid√£o**](#Words-of-Appreciation)

## Engenharia de Dados com Airflow

Neste projeto, utilizamos o Apache Airflow, uma plataforma de c√≥digo aberto para orquestra√ß√£o de workflows, para automatizar e gerenciar o pipeline de ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento) dos dados dos conv√™nios e contratos do Governo do Estado do Cear√°. O Airflow permite definir, agendar e monitorar workflows de forma program√°tica, oferecendo uma interface de usu√°rio que facilita o gerenciamento das tarefas de dados.

### Airflow e Docker

Para garantir a portabilidade e a facilidade na configura√ß√£o do ambiente, o Airflow √© executado dentro de cont√™ineres Docker. O Docker encapsula o Airflow e suas depend√™ncias em cont√™ineres isolados, permitindo que o servi√ßo seja executado de maneira consistente em qualquer ambiente de hospedagem que suporte Docker. Isso simplifica o processo de implanta√ß√£o e configura√ß√£o do Airflow, al√©m de proporcionar a separa√ß√£o entre o ambiente de execu√ß√£o e o sistema operacional subjacente.


### Banco de Dados PostgreSQL com DBeaver

Nossa infraestrutura de back-end foi feita no PostgreSQL, um sistema de banco de dados relacional de c√≥digo aberto renomado por sua robustez e confiabilidade. Para gerenciar e interagir com o nosso banco de dados PostgreSQL, adotamos o DBeaver, um cliente SQL universal que oferece uma interface gr√°fica rica para o gerenciamento de bancos de dados. Com o DBeaver, temos uma ferramenta poderosa para realizar consultas SQL, visualizar a estrutura de dados, desenvolver esquemas de banco de dados e depurar opera√ß√µes, tudo isso contribuindo para um processo de ETL eficiente e confi√°vel no Airflow.

As caracter√≠sticas avan√ßadas do PostgreSQL, como conformidade com ACID, suporte a transa√ß√µes complexas e extensibilidade, fazem dele a escolha ideal para armazenar e manipular os dados cr√≠ticos do nosso projeto. Ao combinar o PostgreSQL com as funcionalidades intuitivas do DBeaver, maximizamos a produtividade e garantimos a manuten√ß√£o eficaz do nosso ambiente de dados.


### DAGs do Airflow

O Airflow utiliza DAGs (Directed Acyclic Graphs) para definir a sequ√™ncia e as depend√™ncias das tarefas de ETL. Cada DAG representa um conjunto de tarefas que s√£o executadas em uma ordem espec√≠fica, permitindo uma complexidade arbitr√°ria na orquestra√ß√£o das tarefas de dados. As DAGs s√£o definidas em Python, o que proporciona flexibilidade e poder na cria√ß√£o de pipelines de dados personalizados para atender √†s necessidades espec√≠ficas do projeto.

### Execu√ß√£o Manual e Agendamento

Embora as DAGs possam ser agendadas para execu√ß√£o em intervalos regulares, tamb√©m oferecemos a op√ß√£o de execu√ß√£o manual. Isso √© √∫til para situa√ß√µes em que uma execu√ß√£o ad-hoc do pipeline √© necess√°ria, como na realiza√ß√£o de testes ou na manipula√ß√£o de dados ap√≥s uma atualiza√ß√£o de esquema. A execu√ß√£o manual √© realizada por meio da interface web do Airflow, fornecendo um meio conveniente e direto para disparar pipelines conforme necess√°rio.

---

Para come√ßar a usar o Airflow com Docker neste projeto, se faz necess√°rio fazer as configura√ß√µes de ambas ferramentas siga as instru√ß√µes detalhadas de configura√ß√£o dispon√≠veis no site do Airflow[https://airflow.apache.org/] e Docker[https://www.docker.com/]. A documenta√ß√£o inclui todas as etapas necess√°rias para inicializar o Airflow em um ambiente Docker e para configurar, executar e monitorar as DAGs de ETL.


### Automa√ß√£o do Pipeline de Dados com Airflow DAGs

A complexidade e a vastid√£o dos dados requerem um sistema robusto para orquestrar o pipeline de ETL, garantindo que as diversas tarefas sejam executadas de forma sequencial e confi√°vel. Para atender a essa necessidade, implementamos Directed Acyclic Graphs (DAGs) no Apache Airflow, estruturando assim um sistema de automa√ß√£o que gerencia a execu√ß√£o de tarefas desde a extra√ß√£o de dados at√© a an√°lise final. Abaixo, detalhamos as quatro DAGs principais que comp√µem nosso pipeline de dados:

#### 1. DAG de Limpeza de Dados (`limpeza_dados`)

- **Objetivo**: Automatizar a exclus√£o de todas as tabelas dos bancos de dados Stage e Data Warehouse. Essa DAG √© particularmente √∫til para resetar o ambiente de dados e prepar√°-lo para novos ciclos de teste ou execu√ß√£o de ETL.
- **Processo**: Executa scripts SQL que limpam os schemas `stage` e `dw`, removendo tabelas e seus dados. Isso assegura um ponto de partida limpo para opera√ß√µes subsequentes, eliminando a necessidade de interven√ß√£o manual e prevenindo inconsist√™ncias de dados.

#### 2. DAG de ETL para o Banco de Dados Stage (`etl_dados_stage`)

- **Objetivo**: Orquestrar a extra√ß√£o de dados das APIs, seguida pela cria√ß√£o de tabelas no banco de dados Stage, inser√ß√£o dos dados extra√≠dos e execu√ß√£o de procedimentos de ETL espec√≠ficos para as tabelas de Conv√™nios e Contratos.
- **Processo**: Esta DAG automatiza desde a coleta de dados at√© a prepara√ß√£o inicial dos mesmos, incluindo a remo√ß√£o de duplicatas baseada em chaves √∫nicas, o tratamento de valores nulos e a normaliza√ß√£o de formatos de texto.

#### 3. DAG de ETL para Dimens√µes e Fatos (`etl_dimensao_fatos`)

- **Objetivo**: Executar os scripts de ETL que conectam ao banco de dados Stage, criam tabelas de dimens√£o e fato no Data Warehouse e inserem dados nessas tabelas.
- **Processo**: Essencial para estruturar o Data Warehouse, essa DAG assegura a correta transfer√™ncia e transforma√ß√£o dos dados do ambiente de Stage para o DW, promovendo a integra√ß√£o entre as diversas dimens√µes e fatos de nosso modelo de dados.

#### 4. DAG de Envio de Email com An√°lises (`enviar_link_analises`)

- **Objetivo**: Enviar automaticamente um email contendo links para a documenta√ß√£o e an√°lises do projeto, oferecendo uma forma r√°pida e eficiente de compartilhar os resultados das an√°lises com stakeholders.
- **Processo**: Essa DAG √© configurada para disparar emails ap√≥s a conclus√£o bem-sucedida das outras DAGs de ETL, garantindo que os interessados sejam informados das √∫ltimas atualiza√ß√µes e an√°lises dispon√≠veis.

### Conclus√£o

As DAGs do Airflow representam uma pe√ßa fundamental na automa√ß√£o e na gest√£o eficiente de nosso pipeline de dados, desde a limpeza e prepara√ß√£o inicial dos dados at√© a an√°lise final e compartilhamento dos insights. Ao implementar essas DAGs, maximizamos a efici√™ncia operacional, reduzimos a possibilidade de erros manuais e garantimos a entrega oportuna de dados confi√°veis e insights valiosos.


### Desenvolvimento dos Scripts Python para Automa√ß√£o de Tarefas

O foco principal da nossa infraestrutura de dados s√£o os scripts Python, que automatizam integralmente as etapas cr√≠ticas de nosso pipeline de dados. Esses scripts s√£o projetados com um foco preciso em efici√™ncia, clareza e reutiliza√ß√£o, permitindo que realizemos desde a extra√ß√£o de dados de APIs externas at√© o carregamento desses dados em nossos bancos de dados Stage e Data Warehouse, passando pelo tratamento e transforma√ß√£o dos dados.

#### Caracter√≠sticas dos Scripts

- **Extra√ß√£o de Dados**: Utilizamos requests HTTP para extrair dados diretamente de APIs p√∫blicas, empregando m√©todos que garantem a obten√ß√£o completa e precisa dos conjuntos de dados necess√°rios.
  
- **Cria√ß√£o Din√¢mica de Tabelas**: Nossos scripts verificam a estrutura dos dados extra√≠dos e, com base nessa estrutura, criam tabelas no banco de dados que se alinham perfeitamente com os formatos dos dados, assegurando assim uma integra√ß√£o sem falhas.

- **Inser√ß√£o e Tratamento de Dados**: Al√©m de inserir os dados nas tabelas correspondentes, nossos scripts implementam v√°rias rotinas de tratamento de dados, incluindo a limpeza de valores nulos, a remo√ß√£o de duplicatas e a normaliza√ß√£o de formatos de dados.

#### Notebooks Jupyter para An√°lise e Educa√ß√£o

Reconhecendo a import√¢ncia da transpar√™ncia e da aprendizagem em projetos de engenharia de dados, tamb√©m disponibilizamos notebooks Jupyter que detalham cada passo do processo de ETL. Estes notebooks servem a dupla fun√ß√£o de documentar nosso processo de maneira interativa e de fornecer uma ferramenta educacional para aqueles que buscam entender melhor as nuances da engenharia de dados e an√°lise de ados.

- **Explora√ß√£o de Dados**: Os notebooks Jupyter fornecem um ambiente excelente para a explora√ß√£o inicial de dados, permitindo a visualiza√ß√£o imediata dos dados extra√≠dos e a execu√ß√£o de an√°lises explorat√≥rias b√°sicas.

- **Tutorial Passo a Passo**: Cada notebook acompanha o leitor atrav√©s das etapas l√≥gicas do processamento de dados, desde a extra√ß√£o e a limpeza at√© a an√°lise preliminar, oferecendo insights sobre as decis√µes tomadas em cada fase.

- **Flexibilidade e Interatividade**: Com a interatividade fornecida pelos notebooks Jupyter, os usu√°rios podem modificar e executar trechos de c√≥digo para experimentar com os dados, proporcionando uma compreens√£o pr√°tica dos princ√≠pios da engenharia de dados.


## Dicion√°rio de Dados Stage

Especifica√ß√£o dos campos e tipos de dados utilizados nas tabelas do banco de dados Stage, com detalhes sobre a origem e o contexto dos dados.

## Cria√ß√£o do Banco de Dados Airflow (Schema Stage)

O Banco de Dados Airflow no Schema Stage foi criado para ser o nosso Banco de Dados Transacional, onde podemos executar as opera√ß√µes de an√°lises dos dados e tratamentos, sem ter que utilizar o banco de produ√ß√£o.
No Stage, temos duas tabelas, que correspondem as tabelas de Conv√™nios e Contratos.

Na execu√ß√£o do Script, cont√©m uma fun√ß√£o que cria um dicion√°rio ds p√°ginas da API, o qual √© poss√≠vel escolher os n√∫meros p√°ginas para extra√ß√£o. Nesse projeto escolhemos 10 p√°ginas.

## An√°lise Explorat√≥ria Inicial no Banco de Dados Stage

Antes de avan√ßar para as complexidades do Data Warehouse, √© essencial realizar uma an√°lise explorat√≥ria abrangente dos dados coletados no banco de dados Stage. Utilizando notebooks Jupyter como nossa ferramenta de investiga√ß√£o, mergulhamos profundamente nos dados para identificar padr√µes, anomalias e insights preliminares. Esta an√°lise inicial √© um passo fundamental para assegurar a qualidade e a relev√¢ncia dos dados antes de sua transforma√ß√£o e carregamento no ambiente do Data Warehouse.

### Pontos Focais da An√°lise

Em nossa explora√ß√£o, damos especial aten√ß√£o a v√°rias dimens√µes cr√≠ticas que influenciam diretamente a utilidade dos dados para an√°lises subsequentes:

- **Valores Nulos**: Identificamos colunas com uma alta incid√™ncia de valores nulos, avaliando o impacto dessas aus√™ncias nos dados e determinando estrat√©gias de imputa√ß√£o ou exclus√£o conforme apropriado. Compreender a natureza dos dados faltantes √© crucial para garantir an√°lises precisas.

- **Registros Duplicados**: A detec√ß√£o e remo√ß√£o de duplicatas s√£o essenciais para evitar distor√ß√µes nas an√°lises. Empregamos t√©cnicas rigorosas para identificar registros duplicados com base em chaves √∫nicas, assegurando a integridade dos dados.

- **Normaliza√ß√£o de Tipos de Dados**: Avaliamos os tipos de dados atribu√≠dos a cada coluna, corrigindo discrep√¢ncias e convertendo dados para formatos mais apropriados para an√°lise. Esta etapa √© vital para facilitar opera√ß√µes de dados subsequentes, especialmente aquelas envolvendo c√°lculos ou compara√ß√µes temporais.

- **An√°lise de Texto e Categoriza√ß√£o**: Examinamos campos de texto para identificar a necessidade de normaliza√ß√£o ou categoriza√ß√£o, o que pode incluir a padroniza√ß√£o de termos, a extra√ß√£o de informa√ß√µes relevantes ou a convers√£o de texto livre em categorias pr√©-definidas.

### Ferramentas e T√©cnicas

A an√°lise √© conduzida com uma combina√ß√£o de ferramentas de processamento de dados e visualiza√ß√£o em Python, incluindo bibliotecas como Pandas para manipula√ß√£o de dados e Matplotlib e Seaborn para visualiza√ß√£o. Essas ferramentas nos permitem explorar de forma eficaz os conjuntos de dados, identificando tend√™ncias, padr√µes e potenciais √°reas de interesse para an√°lises mais profundas no DW.

### Impacto na Modelagem do Data Warehouse

Os insights obtidos nesta fase inicial de an√°lise explorat√≥ria s√£o fundamentais para informar a modelagem do nosso Data Warehouse. Decis√µes sobre a estrutura de tabelas, a necessidade de dimens√µes adicionais e a forma de tratar dados hist√≥ricos s√£o todas influenciadas pelas descobertas feitas durante esta etapa. Ao assegurar uma compreens√£o s√≥lida dos dados em seu estado inicial, podemos projetar um DW que n√£o apenas acomoda os dados de maneira eficiente, mas tamb√©m maximiza sua utilidade para an√°lises complexas e gera√ß√£o de insights.


## Processo de ETL no Banco de Dados Stage

O processo de ETL (Extra√ß√£o, Transforma√ß√£o e Carregamento) √© uma etapa crucial no nosso pipeline de dados, preparando as informa√ß√µes extra√≠das para an√°lises futuras. Utilizando scripts Python detalhadamente elaborados, baseamos nosso tratamento de dados em descobertas e an√°lises preliminares realizadas atrav√©s de notebooks Jupyter. Este processo n√£o s√≥ garante a qualidade e a integridade dos dados no banco de dados Stage, mas tamb√©m estabelece uma base s√≥lida para as etapas subsequentes de modelagem e an√°lise no Data Warehouse.

### Limpeza e Normaliza√ß√£o dos Dados

Nosso script de ETL engloba v√°rias opera√ß√µes essenciais para a prepara√ß√£o dos dados, incluindo:

- **Exclus√£o de Duplicatas**: Identificamos e removemos registros duplicados baseando-nos em chaves √∫nicas espec√≠ficas para cada conjunto de dados. Esta etapa √© fundamental para garantir a unicidade e a precis√£o dos dados analisados.

- **Tratamento de Valores Nulos**: Avaliamos cuidadosamente os campos com valores nulos, aplicando estrat√©gias de tratamento adequadas que variam desde a imputa√ß√£o de valores at√© a exclus√£o de registros, dependendo do contexto e da import√¢ncia dos dados em quest√£o.

- **Normaliza√ß√£o de Tipos de Dados**: Convertendo tipos de dados inadequados para formatos mais apropriados para an√°lise. Um exemplo √© a transforma√ß√£o de datas armazenadas como texto para o formato `TIMESTAMP WITH TIME ZONE`, facilitando opera√ß√µes temporais complexas e aumentando a precis√£o das consultas temporais.

### Consolida√ß√£o da Base de Dados

Ap√≥s o tratamento, os dados s√£o consolidados em um formato estruturado e coerente, pronto para ser transferido para o Data Warehouse. Este processo assegura que o fluxo de dados do Stage para o DW seja suave e eficiente, maximizando a utilidade das informa√ß√µes para an√°lises e decis√µes baseadas em dados.

### Import√¢ncia do Processo de ETL

O meticuloso processo de ETL no banco de dados Stage √© um componente vital do nosso projeto de engenharia de dados. Ele n√£o apenas melhora a qualidade dos dados dispon√≠veis para an√°lise, mas tamb√©m otimiza o desempenho do sistema e apoia decis√µes informadas. Atrav√©s deste processo, estamos comprometidos em fornecer uma base de dados confi√°vel, precisa e acess√≠vel para todos os stakeholders envolvidos no projeto de an√°lise de conv√™nios e contratos do Governo do Estado do Cear√°.


## Dicion√°rio de Dados DW

Defini√ß√£o detalhada dos elementos constituintes do Data Warehouse, com explica√ß√£o sobre a modelagem e relacionamentos entre as tabelas. Nessa etapa algumas colunas foram desconsideradas, por n√£o serem relevantes nas an√°lises ou por conter valores totalmente nulos.

### Modelagem de Dados do Data Warehouse

Para o desenho e a implementa√ß√£o do modelo de dados do nosso Data Warehouse, adotamos uma abordagem estrat√©gica que prioriza a efici√™ncia na an√°lise de dados, a integridade referencial e a escalabilidade. Entendendo a complexidade e a vastid√£o dos dados envolvidos nos conv√™nios e contratos do Governo do Estado do Cear√°, optamos por um modelo que favorece a flexibilidade anal√≠tica e o desempenho em consultas.

#### Ferramenta de Modelagem: DB Designer

Para facilitar a visualiza√ß√£o e o planejamento da nossa arquitetura de dados, escolhemos o DB Designer como nossa ferramenta de modelagem. Este software nos permitiu criar um esquema visual do Data Warehouse, evidenciando as rela√ß√µes entre tabelas, as chaves prim√°rias e estrangeiras, e garantindo que o design do banco de dados esteja alinhado com as melhores pr√°ticas de modelagem dimensional.

#### Estrat√©gia de Modelagem

Nossa modelagem segue o paradigma dimensional, uma escolha que se destaca por sua simplicidade e efic√°cia em ambientes de Data Warehouse. Neste modelo, separamos os dados em duas categorias principais:

- **Dimens√µes (Dim)**: Tabelas que cont√™m atributos descritivos, servindo como pontos de refer√™ncia e filtros nas an√°lises. As dimens√µes criadas (`dim_contrato`, `dim_entidade`, `dim_modalidade`, `dim_participacao`, `dim_projeto`) oferecem uma vis√£o multidimensional dos dados, facilitando o entendimento e a segmenta√ß√£o dos mesmos.

- **Fatos (Fato)**: Tabelas que registram as m√©tricas quantitativas e as transa√ß√µes, essenciais para an√°lises e relat√≥rios. As tabelas `fato_contratos` e `fato_convenios` acumulam os dados num√©ricos e os eventos relacionados a cada contrato e conv√™nio, respectivamente.

#### Justificativa das Decis√µes

A escolha de unificar algumas dimens√µes, enquanto mantemos tabelas fato separadas, foi guiada pela necessidade de simplificar as consultas sem comprometer a riqueza e a precis√£o das an√°lises. Essa estrutura permite que analistas e gestores explorem os dados de maneira intuitiva, cruzando informa√ß√µes entre diferentes dimens√µes para extrair insights precisos e acion√°veis.

O uso do DB Designer, al√©m de otimizar nosso processo de modelagem, assegurou que todas as partes interessadas pudessem compreender e contribuir para o design do Data Warehouse, garantindo que a solu√ß√£o final atendesse plenamente √†s necessidades do projeto.

Ao final, nossa abordagem na constru√ß√£o do modelo de dados do Data Warehouse demonstra um compromisso com a clareza, a efici√™ncia e a adaptabilidade, assegurando que o ambiente anal√≠tico possa evoluir juntamente com as demandas do Governo do Estado do Cear√° e das partes interessadas.


## Estrutura√ß√£o do Data Warehouse (Schema DW)

O Schema DW no nosso banco de dados Airflow foi projetado como estrutura principal do nosso projeto. Inicialmente, consideramos a implanta√ß√£o do DW em uma plataforma cloud, como o Google BigQuery, para aproveitar a escalabilidade e o desempenho. Contudo, devido √†s considera√ß√µes log√≠sticas e ao tempo necess√°rio para a migra√ß√£o de dados, essa ideia permanece como um plano futuro a ser implementado.

No Data Warehouse, estruturamos uma s√©rie de tabelas fundamentais para a organiza√ß√£o e an√°lise dos dados:

- `dim_contrato`: Armazena informa√ß√µes detalhadas sobre cada contrato.
- `dim_entidade`: Cont√©m dados sobre as entidades envolvidas.
- `dim_modalidade`: Registra as modalidades de contratos e conv√™nios.
- `dim_participacao`: Detalha a participa√ß√£o das entidades nos contratos.
- `dim_projeto`: Fornece informa√ß√µes sobre os projetos associados.
- `fato_contratos`: Tabela fato que compila dados transacionais de contratos.
- `fato_convenios`: Similarmente, agrupa informa√ß√µes transacionais de conv√™nios.

Observamos que as estruturas das tabelas de Conv√™nios e Contratos compartilham colunas id√™nticas, o que nos levou a unificar as dimens√µes, enquanto mantivemos tabelas fato distintas para cada tipo de dado. Essa decis√£o estrat√©gica n√£o s√≥ otimizou nosso Data Warehouse ao reduzir a redund√¢ncia de tabelas, mas tamb√©m melhorou a efici√™ncia da consulta de dados, permitindo an√°lises precisas por meio das rela√ß√µes estabelecidas entre as dimens√µes e as tabelas fato.relacionamentos com as dimens√µes. Isos otimiza a nossa Data Warehouse, diminuindo a quantidade de tabelas no banco de dados.


## An√°lise Explorat√≥ria e Estat√≠stica DW

Nesta etapa do projeto, analisamos os dados inseridos no Data Warehouse. Utilizando t√©cnicas de an√°lise de dados e estat√≠ticas com o objetivo de desvendar padr√µes, identificar correla√ß√µes e destacar anomalias nos conv√™nios e contratos gerenciados pelo Governo do Estado do Cear√°.

As an√°lises realizadas forneceram insights valiosos sobre a efic√°cia das pol√≠ticas p√∫blicas, a distribui√ß√£o de recursos e as tend√™ncias de gastos, contribuindo assim para a tomada de decis√µes estrat√©gicas baseadas em evid√™ncias. Al√©m disso, essas investiga√ß√µes estat√≠sticas pavimentaram o caminho para a identifica√ß√£o de oportunidades de melhoria nos processos de governan√ßa e fiscaliza√ß√£o, garantindo maior transpar√™ncia e efici√™ncia na gest√£o dos recursos p√∫blicos.


## Data Visualization

Para dar vida aos dados e facilitar a interpreta√ß√£o das an√°lises realizadas, utilizamos o Power BI. Com ela, criamos dashboards interativos e relat√≥rios detalhados que destacam as tend√™ncias, padr√µes e insights dos conv√™nios e contratos do Governo do Estado do Cear√°.

Explore nosso projeto de BI e mergulhe nas visualiza√ß√µes din√¢micas atrav√©s do [link do nosso projeto no Power BI](www.d.com.br).


## Licen√ßa

Este projeto √© disponibilizado sob a Licen√ßa MIT, que oferece ampla liberdade para uso pessoal e comercial, desde que o c√≥digo original e os direitos autorais sejam mantidos. A Licen√ßa MIT √© reconhecida por sua simplicidade e flexibilidade, permitindo a redistribui√ß√£o e modifica√ß√£o do c√≥digo sob os termos que garantem a atribui√ß√£o adequada ao autor original. Para mais detalhes, consulte o arquivo LICENSE inclu√≠do neste reposit√≥rio.
---

## Como usar este reposit√≥rio

Este reposit√≥rio est√° organizado em v√°rias pastas que correspondem √†s etapas do processo de engenharia de dados e an√°lise de dados. Para reproduzir o projeto, siga os passos contidos em cada pasta, come√ßando pela Engenharia de Dados se for utlizar o Airflow para Configura√ß√£o e Cria√ß√£o das Tabelas no STAGE e DW ou; pode iniciar pela pasta de Cria√ß√£o_DB_Stage que segue extraindo, criando as tabelas e inserindo os dados e na sequencia  a pasta com as an√°lises do banco de dados e DataViz.


## Team:

* **Nayara Valevskii** - *Serving as Data Engineer and Documentation Specialist* - [Data Engineer]  Github (https://github.com/NayaraWakewski) | LinkedIn (https://www.linkedin.com/in/nayaraba/)
* **Isadora Xavier** - *Serving as Data Analyst and Data Visualization Creator* - [Data Analyst]
Github (https://github.com/isadoraxavier) | LinkedIn (https://www.linkedin.com/in/isadora-xavier/)

## üéÅ Words of Appreciation

* Share this project with others üì¢;
* Want to learn more about the project? Get in touch for a :coffee:;

---
‚å®Ô∏è with ‚ù§Ô∏è by [Nayara Vakevskii](https://github.com/NayaraWakewski) üòä
